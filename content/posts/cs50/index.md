---
title: "CS50"
subtitle:
draft: false
date: 2024-06-19
description:
keywords:
license:
comment: false
weight: 0
tags:
  - draft
categories:
  - draft
hiddenFromHomePage: false
hiddenFromSearch: false
hiddenFromRss: false
hiddenFromRelated: false
summary:
resources:
toc: true
math: true
lightgallery: false
password:
message:

# See details front matter: https://fixit.lruihao.cn/documentation/content-management/introduction/#front-matter
---

<!--more-->


## Search

ç†Ÿæ‚‰çš„è¯é¢˜ï¼Œä¸è¿‡ä¾ç„¶è®©äººå›å¿†èµ·ä¸€ä¸ªå®¹æ˜“å¿½è§†çš„ç‚¹ï¼šä½¿ç”¨ stack è¿›è¡Œè¿­ä»£å¼ dfsã€‚

### GBFS

ä½¿ç”¨å¯å‘å‡½æ•° heuristic h(s)

### A*

åŒæ—¶è€ƒè™‘åˆ°è¾¾æŸä¸ªçŠ¶æ€çš„èŠ±è´¹ g(s) å’Œå¯å‘å‡½æ•° h(s)ã€‚

ä¸‹é¢çš„å›¾ä¸¾ä¾‹äº†ä¸€ä¸ª A* å’Œ GBFS çš„ä¸åŒï¼š
<img src="attachment/c1ad54f5030f2a6fb65823f4cd27ec05.png" />
å½“å·²ç»è®¿é—®è¿‡äº®è‰²çš„ä½ç½®åï¼ŒA* æ€»æ˜¯é€‰æ‹© h(s) = 13 çš„ä½ç½®ä½œä¸ºä¸‹ä¸€ä¸ªè®¿é—®çš„ä½ç½®ã€‚

å½“æ»¡è¶³ï¼š
1. heuristic å‡½æ•°ä»ä¸é«˜ä¼°ä»£ä»·
2. heuristic å‡½æ•°å…·æœ‰ä¸€è‡´æ€§ï¼ˆå¯¹ä»»æ„ $n \xrightarrow{cost:c} n'$ ï¼Œ$h(n) <= h(n') + c$ï¼‰
æ—¶ï¼ŒA* ç»™å‡ºæœ€ä¼˜è§£ã€‚

### alpha beta å‰ªæ

<img src="attachment/4088f88184f17bf6b8843a605be98db8.png" />



## Knowledge

### Project: minesweeper

æµ‹è¯•æ•°æ®åœ¨ https://github.com/ai50/projects çš„å¯¹åº”åˆ†æ”¯ã€‚

minesweeper ä¸­ Sentence çš„ spec æ²¡æœ‰å¾ˆå‡†ç¡®çš„è§„å®šä¸€äº›ç»†èŠ‚ï¼Œå¯ä»¥ç»“åˆæµ‹è¯•æ•°æ®å»å†™ã€‚

æœ€ä¸»è¦çš„éƒ¨åˆ†æ˜¯ `MinesweeperAIã€‚add_knowledge`ï¼Œæ ¹æ®æ³¨é‡Š

```python
'''
This function should:

1) mark the cell as a move that has been made

2) mark the cell as safe

3) add a new sentence to the AI's knowledge base based on the value of `cell` and `count`

4) mark any additional cells as safe or as mines if it can be concluded based on the AI's knowledge base

5) add any new sentences to the AI's knowledge base if they can be inferred from existing knowledge
'''
```

å…¶ä¸­ï¼Œç®€å•å®ç°ï¼š 4) å’Œ 5) ä¸æ–­å¾ªç¯ç›´åˆ°å¾—ä¸åˆ°æ–°çš„æ¨è®ºã€‚

##  Uncertainty

### å‡ ä¸ª ruleï¼š

**æ¡ä»¶æ¦‚ç‡**(conditional probability)ï¼š

$$
P(AB) = P(A)P(B|A)=P(B)P(A|B)
$$

AB åŒæ—¶å‘ç”Ÿï¼šA å‘ç”Ÿï¼›å·²çŸ¥ A å‘ç”Ÿï¼ŒBå‘ç”Ÿï¼Œä¸¤è€…ä¹˜ç§¯ã€‚

å˜å½¢å¾—åˆ°è´å¶æ–¯å®šå¾‹ï¼š

$$
P(B|A)=\frac{P(B)P(A|B)}{P(A)}
$$

**Marginalization**:

$$
P(A) = P(A,B) + P(A, \neg B)
$$

**conditioning**:

$$
P(A) = P(A|B)P(B) + P(A|\neg B)P(\neg B)
$$

### Project: Bayesian network

Wikimediaï¼šhttps://zh.wikipedia.org/wiki/%E8%B2%9D%E6%B0%8F%E7%B6%B2%E8%B7%AF

ç®€å•ç†è§£ï¼šäº’ç›¸å½±å“çš„éšæœºå˜é‡çš„ DAGï¼Œæœ‰å‘è¾¹ $u \to v$ è¡¨ç¤º $v$ å— $u$ å½±å“ã€‚

ç½‘ç»œä¸­çš„è¾¹åˆ»ç”»äº†éšæœºå˜é‡è§çš„ä¾èµ–ï¼Œä¸‹é¢æ˜¯ç½‘ç»œä¸­è®¡ç®—è”åˆæ¦‚ç‡çš„ä¸€ä¸ªä¾‹å­ï¼š
<img src="attachment/278696fa9753714b80291c2d3f924a72.png" />

### project heredity

æ•°å­¦è‹¦æ‰‹å¼€å§‹æ‰å¤´å‘äº†ğŸ’”ã€‚è™½ç„¶ project å®Œæˆçš„ã€Œè¿˜ç®—é¡ºåˆ©ã€ï¼Œä¸è¿‡ç ”ç©¶ spec èŠ±äº†å¾ˆä¹…ï¼Œä¸­é—´å¯¹ `joint_probability` ç†è§£å‡ºé”™æ˜¾ç„¶æš´éœ²äº†å’±å¯¹ lecture ä¸­è¿™ä¸€éƒ¨åˆ†ç†è§£æœ‰é—®é¢˜ï¼Œä½†æ˜¯å’±æœ‰ä»€ä¹ˆåŠæ³•ï¼Œå’±ä¹ŸçŸ¥é“è‡ªå·±è¿·è¿·ç³Šç³Šçš„ğŸ˜µã€‚

å›åˆ° projï¼Œå®é™…ä¸Šï¼Œè¿™ä¸ª proj åšçš„äº‹æƒ…å°±æ˜¯å®ç°ä¸€ä¸ª Notes ä¸­çš„ [Inference by Enumeration](https://cs50.harvard.edu/ai/2024/notes/2/)ã€‚ä¸ºäº†ç†è§£ heredity.pyï¼Œå…ˆå…³æ³¨ä¸‹è¿™å‡ ä¸ªé—®é¢˜ï¼š

#### `joint_probability` çš„å®ç°ã€‚

å·²çŸ¥å…¨éƒ¨çš„åå­—ï¼Œä»¥åŠæ¯ä¸ªäººæœ‰å‡ ä¸ªç›®æ ‡åŸºå› ä»¥åŠæ¯ä¸ªäººæ˜¯å¦è¡¨ç°å‡ºç—‡çŠ¶ï¼Œæˆ‘ä»¬è¦ç®—çš„æ˜¯è¿™äº›äº‹æƒ…å…¨éƒ¨åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚

åæ€è‡ªå·±ï¼Œæˆ‘è§‰å¾—è‡ªå·±æœ€å¤§çš„é—®é¢˜æ˜¯ä¾ç„¶æ²¡æœ‰æŠŠäº‹ä»¶é—´æ¥çš„å½±å“ç»™æ’é™¤æ‰ï¼Œå†æ ‡è®°ä¸‹ Notes ä¸­çš„ä¸€æ®µåŸæ–‡ï¼š**This point about Bayesian network is noteworthy: parents include only direct relations.** åœ¨è€ƒè™‘äº‹ä»¶ä¹‹é—´çš„å½±å“æ—¶ï¼Œåªè€ƒè™‘ç›´æ¥å½±å“ï¼

ä»¤è¿”å›å€¼ r çš„åˆå§‹å€¼ä¸º 1ï¼Œé‚£ä¹ˆæ•°æ®ä¸­æ²¡æœ‰è®°å½•çˆ¶æ¯çš„äººï¼ŒæŒ‰ç…§è¦æ±‚ä½¿ç”¨ PROBS ä¸­çš„æ•°æ®è®¡ç®—æ¦‚ç‡ï¼Œä¹˜å…¥ r ä¸­å³è¿™äº›äººå¯¹ r çš„è´¡çŒ®ã€‚é‚£ä¹ˆåœ¨è´å¶æ–¯ç½‘ç»œä¸­å‘ä¸‹ä¸€å±‚ï¼Œå‰é¢é‚£äº›äººçš„å­å¥³ï¼Œä»–ä»¬å¯¹ r çš„è´¡çŒ®å³ï¼šåœ¨çˆ¶æ¯ç¬¦åˆå‚æ•°æè¿°çš„æ¡ä»¶ä¸‹ï¼Œä»–ä»¬ç¬¦åˆå‚æ•°ä¸­æè¿°çš„çŠ¶æ€çš„æ¦‚ç‡ã€‚è¿™äº›æ¦‚ç‡åªéœ€è¦æŒ‰ç…§çˆ¶æ¯çš„æƒ…å†µï¼Œç®€å•ä¹˜æ³•è®¡ç®—å³å¯ã€‚ä¹‹åç»§ç»­åœ¨ç½‘ç»œä¸­å‘ä¸‹ï¼ŒåŒç†ã€‚

è™½ç„¶è¿™é‡Œçš„æ€è€ƒè¿‡ç¨‹æ˜¯æŒ‰ç…§å›¾æœ‰å‘è¾¹çš„é¡ºåºè¿›è¡Œçš„ï¼Œä½†æ˜¯è®¡ç®—æ¯ä¸ªäººå¯¹ r çš„è´¡çŒ®ä¾èµ–çš„çˆ¶æ¯çš„æƒ…å†µæ˜¯ä½œä¸ºæ¡ä»¶å·²çŸ¥çš„ï¼Œè®¡ç®—å¹¶ä¸éœ€è¦æŒ‰ç…§ç‰¹å®šé¡ºåºã€‚

```python
def joint_probability(
    people: dict[str, dict[Literal["mother", "father"], str | None]],
    one_gene: set[str],
    two_genes: set[str],
    have_trait: set[str],
):
    """
    Compute and return a joint probability.

    The probability returned should be the probability that
        * everyone in set `one_gene` has one copy of the gene, and
        * everyone in set `two_genes` has two copies of the gene, and
        * everyone not in `one_gene` or `two_gene` does not have the gene, and
        * everyone in set `have_trait` has the trait, and
        * everyone not in set` have_trait` does not have the trait.
    """
    def p_get_from(n: Literal[0, 1], n_parent: Literal[0, 1, 2]):
        '''
        probability of get n genes from one parent of n_parent genes
        '''
        m, nm = PROBS["mutation"], 1 - PROBS["mutation"]
        table = {
            0: {
                0: nm, # parent 0 gene, so as long as no mutation we got 0
                1: 0.5, # = 0.5*nm + 0.5*m
                2: m,
            },
            1: {
                0: m,
                1: 0.5,
                2: nm,
            },
        }

        assert n in [0, 1] and n_parent in [0, 1, 2]
        return table[n][n_parent]

    j_prob = 1
    for name in people.keys():
        ng_of = lambda name: 1 if name in one_gene else 2 if name in two_genes else 0

        if people[name]["mother"] == None and people[name]["father"] == None:
            j_prob *= PROBS["gene"][ng_of(name)]
        elif people[name]["mother"] != None and people[name]["father"] != None:
            this_ng = ng_of(name)
            mng = ng_of(people[name]["mother"])
            fng = ng_of(people[name]["father"])

            if this_ng == 0:
                j_prob *= p_get_from(0, mng) * p_get_from(0, fng)
            elif this_ng == 1:
                j_prob *= p_get_from(1, mng) * p_get_from(
                    0, fng
                ) + p_get_from(0, mng) * p_get_from(1, fng)
            else:
                j_prob *= p_get_from(1, mng) * p_get_from(1, fng)
        else:
            raise "invalid data: one of father and mother is None"

        is_have_trait = name in have_trait
        j_prob *= PROBS["trait"][ng_of(name)][is_have_trait]

    return j_prob
```

#### main çš„å®ç°

main ä¸»è¦éƒ¨åˆ†å°±æ˜¯è®¡ç®—

$$
P(X|e) = \alpha P(X,e) = \alpha \sum_{y}P(X,e,y)
$$

çš„é€»è¾‘ã€‚ä¸‰ä¸ª for ä¸ä»…æ˜¯åœ¨éå† $y$ï¼Œä¹Ÿæ˜¯åœ¨éå† $X$ï¼Œåé¢ `update` ä¸­çš„ forï¼Œå¯ä»¥è®¤ä¸ºæ˜¯åœ¨éå†ç»“æœä¸­åˆ’åˆ† $X$ å’Œ $y$ï¼Œåˆ’åˆ†å®Œæˆåè®¡ç®—é‚£ä¸€æ¬¡éå†å¯¹åº”çš„æ¯ä¸ªäººçš„æƒ…å†µå¯¹æ¯ç§ $X$ çš„è´¡çŒ®ã€‚

## Optimization

### Hill Climbing

å¯ä»¥ä½¿ç”¨å¤šç§é€‰æ‹©æ›´ä¼˜çš„ç›¸é‚»å€¼çš„ç­–ç•¥ã€‚

### Simulated Annealingã€Linear Programming

### Constraint Satisfaction problems

-  AÂ **Hard Constraint**Â is a constraint that must be satisfied in a correct solution.
- AÂ **Soft Constraint**Â is a constraint that expresses which solution is preferred over others.
- AÂ **Unary Constraint**Â is a constraint that involves only one variable. In our example, a unary constraint would be saying that course A canâ€™t have an exam on Monday {_A â‰  Monday_}.
- AÂ **Binary Constraint**Â is a constraint that involves two variables. This is the type of constraint that we used in the example above, saying that some two courses canâ€™t have the same value {_A â‰  B_}.

#### Node consistency

Node consistency is when all the values in a variableâ€™s domain satisfy the variableâ€™s unary constraints.

#### Arc Consistency

Arc consistency is when all the values in a variableâ€™s domain satisfy the variableâ€™s binary constraints (note that we are now using â€œarcâ€ to refer to what we previously referred to as â€œedgeâ€).

## Machine Learning

Machine learning provides a computer with data, rather than explicit instructions. Using these data, the computer learns to recognize patterns and becomes able to execute tasks on its own.

### Supervised learning

**Supervised learning** is a task where a computer learns a function that maps inputs to outputs based on a dataset of input-output pairs.

There are multiple tasks under supervised learning, and one of those isÂ **Classification**: Nearest-Neighbor Classification, Perceptron Learning, Support Vector Machines.

**Regression** is a supervised learning task of a function that maps an input point to a continuous value, some real number. This differs from classification in that classification problems map an input to discrete values.

**Regularization** is the process of penalizing hypotheses that are more complex to favor simpler, more general hypotheses. We use regularization to avoid **overfitting**.

In regularization, we estimate the cost of the hypothesis function h by adding up its loss and a measure of its complexity.

_cost_(h) =Â _loss_(h) + Î»_complexity_(h)

**Reinforcement learning** is another approach to machine learning, where after each action, the agent gets feedback in the form of reward or punishment (a positive or a negative numerical value).

Reinforcement learning can be viewed as a Markov decision process, having the following properties:
- Set of statesÂ **_S_**
- Set of actionsÂ **_Actions(S)_**
- Transition modelÂ **_P(sâ€™ | s, a)_**
- Reward functionÂ **_R(s, a, sâ€™)_**

**Q-Learning** is one model of reinforcement learning, where a functionÂ **_Q(s, a)_**Â outputs an estimate of the value of taking actionÂ _a_Â in stateÂ _s_.

### Unsupervised Learning

In supervised learning, we had data with labels that the algorithm could learn from. In unsupervised learning, only the input data is present and the AI learns patterns in these data.

**Clustering** is an unsupervised learning task that takes the input data and organizes it into groups such that similar objects end up in the same group.

## Neural Networks

**A** neural network: a function sums up inputs to produce an output. Example: inputs $x_1, x_2$, with some weights and a bias: $w_0, w_1, w_2$ and a activation function $g$. We got an output: $g(w_0+w_1x_1+w_2x_2)$.

> quiz: How many total weights (including biases) will there be for a fully connected neural network with a single input layer with 3 units, a single hidden layer with 5 units, and a single output layer with 4 units?
>
> Each unit of 5 hidden layer units need 3 + 1(bias) weights.
> Each unit of 4 output layer units need 5 + 1(bias) weights.
> Total:
> $5 \times (3+1) + 4 \times (5+1) = 20 + 24 = 44$

**deep neural networks**: neural networks that have more than one hidden layer.

A **convolutional neural network** is a neural network that uses convolution, usually for analyzing images.

As opposed to those **Feed-Forward Neural Networks** where input data is provided to the network, which eventually produces some output, **Recurrent Neural Networks** consist of a non-linear structure, where the network uses its own output as input.

Feed-Forward Neural Network:
<img src="attachment/f3e1514a1bcbb6aabaac09706804f1d8.png" />

RNN:
<img src="attachment/3430d9b6471222a3b32e899cb24d0e3e.png" />

### Project

ä¸ä½¿ç”¨ GPU çš„è¯ tensorflow åŸºæœ¬æ˜¯å¼€ç®±å³ç”¨çš„ï¼Œä½†æ˜¯å…¨ç¨‹ç”¨ CPU è®­ç»ƒæ¨¡å‹çš„æ•ˆç‡å®åœ¨æ˜¯æœ‰ç‚¹å·®ï¼Œåˆšå¥½æˆ‘æ‰‹è¾¹åæ­£æœ‰å— 40 ç³»æ˜¾å¡ï¼Œä¸è¿‡ Arch ä¸‹é…ç½®è¿™ä¸ªè¿è¡Œç¯å¢ƒè¿˜æ˜¯æœ‰ç‚¹çƒ¦çš„å•Šã€‚

Archwiki çš„ GPGPU æ¡ç›®ä»‹ç»äº† cuda ç›¸å…³çš„åŒ…ï¼Œä½†æ˜¯æŒ‰ç…§ä¸Šé¢çš„è¯´æ˜åšå®Œä¹‹åï¼Œè¿è¡Œ tensorflow çš„æµ‹è¯•ï¼Œè¢«æç¤º
```plaintext
Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide atÂ [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)Â for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
é”™è¯¯ä¿¡æ¯æç¤º ã€Œmake sure the missing libraries mentioned above are installed properlyã€ï¼Œä½†æ˜¯ä»ä¸Šæ–‡å¹¶æ²¡æœ‰çœ‹åˆ°æœ‰æç¤ºä»€ä¹ˆåº“ç¼ºå¤±çš„ä¿¡æ¯ã€‚é€šè¿‡ `export LD_DEBUG=libs` æ£€æŸ¥å¯ä»¥çŸ¥é“ tensorflow å°è¯•æ‰“å¼€ libcudnn ç‰ˆæœ¬ 8ï¼Œä½†æ˜¯åœ¨ç¡®ä¿ç³»ç»Ÿæœ€æ–°çš„æƒ…å†µä¸‹ï¼Œcudnn å·²ç»åœ¨ç‰ˆæœ¬ 9 äº†ï¼Œä»è€Œå¯¼è‡´äº†ä¸Šé¢çš„å¤±è´¥ã€‚

tensorflow å®˜ç½‘æœ‰å¦ä¸€ç§å®‰è£…æ–¹å¼ï¼šdockerã€‚æˆ‘è¿™é‡Œä½¿ç”¨ podman ä½œä¸ºå®¹å™¨å®ç°ï¼Œé¦–å…ˆä¾ç…§ [Installing the NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit "Permalink to this headline") å®‰è£… NVIDIA Container Toolkitï¼Œç„¶å[é…ç½® CDI](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html)ï¼Œæœ€å pullã€run å®Œäº‹ã€‚ä¾¿åˆ©ï¼

```shell
podman pull docker.io/tensorflow/tensorflow:latest-gpu
podman run --gpus all -dit tensorflow/tensorflow:latest-gpu bash
```
